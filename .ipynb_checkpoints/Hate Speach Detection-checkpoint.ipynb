{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e3e17d-dca0-47fb-bb08-b937506f2ec7",
   "metadata": {},
   "source": [
    "# Hate Speach Detection\n",
    "I want to build an algorithim to detect hate speach.  This will use the Hate Speach and Offensive Language dataset at https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset?select=labeled_data.csv.  This dataset is 24782 tweets long with users voting on if the tweet is offensive, hate_speach or neither, with the winning label being selected as a class.\n",
    "## Approach\n",
    "I will approach this as a text classification problem similar to the BBC problem.  The big difference is that my text can be labeled as either hate speach, or offensive language.  Users were allowed to vote on which category the data fell into but were only permitted to select 1 category when multiple may apply.\n",
    "\n",
    "I'm going to move the count to a boolean selection.  If either hate speach or offensive language have more than 1/3 of the count, it will be 1, otherwise it will be 0.  After I perform NMF, I will use various models to bucket the new text.  For classification, I will generate an expected value and test various cutoffs for classification.\n",
    "\n",
    "### Disclaimer\n",
    "This assignment necissarly contains hate speach, and offensive langauge.  This does not represent the views of the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c222a22a-d816-4d91-b317-0908f884f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a7927248-8812-484d-97e0-e26c4530a697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "1            1      3            0                   3        0      1   \n",
      "2            2      3            0                   3        0      1   \n",
      "3            3      3            0                   2        1      1   \n",
      "4            4      6            0                   6        0      1   \n",
      "5            5      3            1                   2        0      1   \n",
      "6            6      3            0                   3        0      1   \n",
      "7            7      3            0                   3        0      1   \n",
      "8            8      3            0                   3        0      1   \n",
      "9            9      3            1                   2        0      1   \n",
      "10          10      3            0                   3        0      1   \n",
      "11          11      3            0                   3        0      1   \n",
      "12          12      3            0                   2        1      1   \n",
      "13          13      3            0                   3        0      1   \n",
      "14          14      3            1                   2        0      1   \n",
      "15          15      3            0                   3        0      1   \n",
      "16          16      3            0                   3        0      1   \n",
      "17          17      3            1                   2        0      1   \n",
      "18          18      3            0                   3        0      1   \n",
      "19          19      3            0                   3        0      1   \n",
      "\n",
      "                                                tweet  \n",
      "1   !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2   !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3   !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4   !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "5   !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...  \n",
      "6   !!!!!!\"@__BrighterDays: I can not just sit up ...  \n",
      "7   !!!!&#8220;@selfiequeenbri: cause I'm tired of...  \n",
      "8   \" &amp; you might not get ya bitch back &amp; ...  \n",
      "9   \" @rhythmixx_ :hobbies include: fighting Maria...  \n",
      "10  \" Keeks is a bitch she curves everyone \" lol I...  \n",
      "11                 \" Murda Gang bitch its Gang Land \"  \n",
      "12  \" So hoes that smoke are losers ? \" yea ... go...  \n",
      "13      \" bad bitches is the only thing that i like \"  \n",
      "14                            \" bitch get up off me \"  \n",
      "15                    \" bitch nigga miss me with it \"  \n",
      "16                             \" bitch plz whatever \"  \n",
      "17                          \" bitch who do you love \"  \n",
      "18                 \" bitches get cut off everyday B \"  \n",
      "19                 \" black bottle &amp; a bad bitch \"  \n"
     ]
    }
   ],
   "source": [
    "#Initial Import\n",
    "raw_data = pd.read_csv('C:\\\\Users\\\\isaac\\\\OneDrive\\\\School\\\\'\\\n",
    "           'MS\\\\Unstructured Learning\\\\Week 5\\\\'\\\n",
    "           'Unsupervised-Learning-Final\\\\Data\\\\'\\\n",
    "           'labeled_data.csv', header = 0)\n",
    "print(raw_data[1:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0eebb-45e3-4012-9973-88a5b302d2b1",
   "metadata": {},
   "source": [
    "## Import and Cleaning\n",
    "### Import\n",
    "This data imports fairly easily.  I donloaded a copy of the file from kaggle and mirrored it to my GitHub so I could autodownload it for your use.\n",
    "### Cleaning\n",
    "#### **Tweets with newline**\n",
    "There appears to be a mismatch between the number of lines generated and the number of lines imported.  This is caused by newline characters, commas, quotation marks, and other supidity mixed in.  I chose to build my own parser to deal with all the issues and allow me to deal with the various problems with the data.\\\n",
    "**Update**\\\n",
    "There are skipped tweet numbers with no indicator in the dataset other than looking.  I'm going to keep my work on my own generator due to the risk avoidance in read_csv with the multitude of special characters.\n",
    "\n",
    "##### **Some examples of problematic formating.**\n",
    "22422,3,0,3,0,1,\"This bitch fell straight through the chair... Im high, of course I laughed..\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "Even if I wasn't I would've still laughed.\"\n",
    "\n",
    "256,3,0,3,0,1,\"\"\"@TheNewSAT: #NewSATQuestions\\\n",
    "Yeah bitch, yeah bitch, call me _______:\\\n",
    "a.) Maybe\\\n",
    "b.) Steve-O\\\n",
    "c.) Later\\\n",
    "d.) Jesse Pinkman\"\"\\\n",
    "@machinegunkelly\"\n",
    "\n",
    "\n",
    "#### **Username Cleanup**\n",
    "I chose to just drop all usernames.  This is due to usernames being random characters and it's out of scope for this project to try to do abusive username detection.\n",
    "#### **Unicode Characters, numbers, and non-text characters**\n",
    "I also chose to drop all unicode characters, numbers and most non-text characters.  The exception was the \"*\" character.  The * character is used to mask offensive words and when used, the resulting word length is fairly standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e5093c7e-fbd8-468e-ab66-ed5ff5cfb432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tweet num count hate_speech offensive_language neither class  \\\n",
      "0             0     3           0                  0       3     2   \n",
      "1             1     3           0                  3       0     1   \n",
      "2             2     3           0                  3       0     1   \n",
      "3             3     3           0                  2       1     1   \n",
      "4             4     6           0                  6       0     1   \n",
      "...         ...   ...         ...                ...     ...   ...   \n",
      "24778     25291     3           0                  2       1     1   \n",
      "24779     25292     3           0                  1       2     2   \n",
      "24780     25294     3           0                  3       0     1   \n",
      "24781     25295     6           0                  6       0     1   \n",
      "24782     25296     3           0                  0       3     2   \n",
      "\n",
      "                                                    text  \n",
      "0       rt as a woman you shouldn t complain about cl...  \n",
      "1       rt boy dats cold tyga dwn bad for cuffin dat ...  \n",
      "2       rt you ever fuck a bitch and she start to cry...  \n",
      "3                  rt viva based she look like a tranny   \n",
      "4       rt the shit you hear about me might be true o...  \n",
      "...                                                  ...  \n",
      "24778   you s a muthaf***in lie pearls corey emanuel ...  \n",
      "24779   you ve gone and broke the wrong heart baby an...  \n",
      "24780  young buck wanna eat dat nigguh like i aint fu...  \n",
      "24781             youu got wild bitches tellin you lies   \n",
      "24782   ruffled ntac eileen dahlia beautiful color co...  \n",
      "\n",
      "[24783 rows x 7 columns]\n",
      "Empty DataFrame\n",
      "Columns: [tweet num, count, hate_speech, offensive_language, neither, class, text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "col_names = [\"tweet num\", \"count\", \"hate_speech\", \"offensive_language\", \"neither\", \"class\",\"text\"]\n",
    "raw_data = pd.DataFrame(columns = col_names)\n",
    "#Open File\n",
    "#url = \"https://raw.githubusercontent.com/isaac1987a/Unsupervised-Learning-Final/main/Data/labeled_data.csv?token=GHSAT0AAAAAACF7NRVKZY7SGGBDHLM25WYAZGP6AXA\"\n",
    "#r = requests.get(url, allow_redirects=True)\n",
    "#replace multiple newlines\n",
    "#buf = io.StringIO(r.text)\n",
    "#print(r.text)\n",
    "#buf = io.StringIO(re.sub(\"\\n\\n+\", \" \", r.text))\n",
    "buf = open('C:\\\\Users\\\\isaac\\\\OneDrive\\\\School\\\\'\\\n",
    "           'MS\\\\Unstructured Learning\\\\Week 5\\\\'\\\n",
    "           'Unsupervised-Learning-Final\\\\Data\\\\'\\\n",
    "           'labeled_data.csv', 'r')\n",
    "Lines = buf.readlines()\n",
    "i = 0\n",
    "for line in Lines:\n",
    "    if i == 0:\n",
    "        i += 1\n",
    "        None\n",
    "    #If line is good\n",
    "    elif re.search(\"[0-9]{1,5},([0-9],){5}\", line) != None:\n",
    "        #Extract the leading values\n",
    "        values = re.match(\"[0-9]{1,5},([0-9],){5}\", line)\n",
    "        #extract the text\n",
    "        text = re.split(\"[0-9]{1,5},([0-9],){5}\", line, maxsplit = 1)[2]\n",
    "        text = text_cleanup(text)\n",
    "        #extract the indevedual values into a thingy\n",
    "        values = re.split(\",\", values.group())\n",
    "        values[6] = text\n",
    "        newline = pd.DataFrame([values],index = [i-1], columns = col_names)\n",
    "        #print(newline)\n",
    "        raw_data = pd.concat([raw_data, newline], axis = 0)\n",
    "        i += 1\n",
    "    #else:\n",
    "        #print(line)\n",
    "        #raw_data['text'].iloc[-1] = text_cleanup(raw_data['text'].iloc[-1] + \" \" + line)\n",
    "        #print(raw_data['text'].iloc[-1])\n",
    "        #print(raw_data['text'])\n",
    "        None\n",
    "       \n",
    "    #shortened test code\n",
    "    #if i > 100:\n",
    "        #break\n",
    "#raw_data = raw_data.drop(columns = [\"tweet num\"])\n",
    "print(raw_data)\n",
    "\n",
    "        \n",
    "def text_cleanup(text):\n",
    "    #Remove usernames replace @xxx: with whitespace\n",
    "    text = re.sub(\"@.*?:\", \" \", text)\n",
    "    #remove unicode characters\n",
    "    text = re.sub(\"&#[0-9]{6};\", \" \", text)\n",
    "    #lowercase all text\n",
    "    text = text.lower()\n",
    "    #remove URL's\n",
    "    text = re.sub('http[s]?:\\/\\/.*?\"', \" \", text)\n",
    "    #remove special characters and numberrs'\n",
    "    #I chose to leave in *'s as they are a common manipulation to get around filters\n",
    "    text = re.sub(\"[^a-z\\*]\", \" \", text)\n",
    "    #remove repeat whitespace and newlines\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "    text = re.sub(\"\\n+\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0b83e21c-87b7-4108-a875-e520d58d13c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "#Find why I still only have 24783 coumns when the dataset says it has 25.3k\n",
    "for i in range(0,1000):\n",
    "    if int(raw_data['tweet num'][i]) != i:\n",
    "        print (i)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5654180a-cb07-49e9-818e-c41bfa86207d",
   "metadata": {},
   "source": [
    "## EDA Procedure\n",
    "### Formatting Data\n",
    "I started with formatting my data so that hate speach, offensive language values are 1 if they have at least 1/3 of the votes, otherwise they will be 0.\n",
    "### Bagging\n",
    "I chose to use TF-IDF for my bagging procedure.  I will do my testing while removing common english stopwords.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93127158-5bb9-48c5-b597-b2c7dc22bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF IDF\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
