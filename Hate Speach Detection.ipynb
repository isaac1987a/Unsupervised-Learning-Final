{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hate Speech Detection\n",
    "\n",
    "I want to build an algorithim to  categorize tweets.  This will use the Hate Speach and Offensive Language dataset at [https://www.kaggle.com/datasets/mrmorj/hate\\-speech\\-and\\-offensive\\-language\\-dataset?select=labeled\\_data.csv](https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset?select=labeled_data.csv).  This dataset is 24782 tweets long with users voting on if the tweet is offensive 0, hate speach 1 or neither 2, with the winning label being selected as a class.  \n",
    "\n",
    "## Goal\n",
    "\n",
    "My goal is to identify groupings of tweets based on topic.  With 27k+  tweets, I should be able to find 30-50 groups based on topics.\n",
    "\n",
    "## Approach\n",
    "\n",
    "I am planning to use a combination of TF-IDF, matrix factorization, and clustering algorithims to tweets by 1 and 2 word clusters.  I will then create similarity tables and use rainbow clustering to find groups of tweets.\n",
    "\n",
    "### Disclaimer\n",
    "\n",
    "This assignment necessary contains hate speach, and offensive langauge.  This does not represent the views of the author.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#General Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import statistics\n",
    "from collections import Counter\n",
    "#EDA Imports\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import sample\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "#Model build Imports\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0               0      3            0                   0        3      2   \n",
      "1               1      3            0                   3        0      1   \n",
      "2               2      3            0                   3        0      1   \n",
      "3               3      3            0                   2        1      1   \n",
      "4               4      6            0                   6        0      1   \n",
      "...           ...    ...          ...                 ...      ...    ...   \n",
      "24778       25291      3            0                   2        1      1   \n",
      "24779       25292      3            0                   1        2      2   \n",
      "24780       25294      3            0                   3        0      1   \n",
      "24781       25295      6            0                   6        0      1   \n",
      "24782       25296      3            0                   0        3      2   \n",
      "\n",
      "                                                   tweet  \n",
      "0      !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "...                                                  ...  \n",
      "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...  \n",
      "24779  you've gone and broke the wrong heart baby, an...  \n",
      "24780  young buck wanna eat!!.. dat nigguh like I ain...  \n",
      "24781              youu got wild bitches tellin you lies  \n",
      "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...  \n",
      "\n",
      "[24783 rows x 7 columns]\n",
      "85.43606504458701\n"
     ]
    }
   ],
   "source": [
    "#Initial Import\n",
    "raw_data = pd.read_csv('Data/labeled_data.csv', header = 0)\n",
    "print(raw_data)\n",
    "raw_data = raw_data.drop(columns = 'Unnamed: 0')\n",
    "print(statistics.mean(raw_data['tweet'].str.len()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import and Cleaning\n",
    "\n",
    "### Import\n",
    "\n",
    "This data imports fairly easily.  I donloaded a copy of the file from kaggle and mirrored it to my GitHub so I could autodownload it for your use.\n",
    "\n",
    "### Data Description\n",
    "The data concists of 24782 tweets which have been user labeled as either hate, offensive, or neither, along with a total cont of votes.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "#### **Tweets with newline**\n",
    "\n",
    "There appears to be a mismatch between the number of lines generated and the number of lines imported.  This is caused by newline characters, commas, quotation marks, and other supidity mixed in.  I chose to build my own parser to deal with all the issues and allow me to deal with the various problems with the data.\\\n",
    "**Update**\\\n",
    "There are skipped tweet numbers with no indicator in the dataset other than looking.\n",
    "\n",
    "##### **Some examples of problematic formating.**\n",
    "\n",
    "22422,3,0,3,0,1,\"This bitch fell straight through the chair... Im high, of course I laughed..\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "Even if I wasn't I would've still laughed.\"\n",
    "\n",
    "256,3,0,3,0,1,\"\"\"@TheNewSAT: #NewSATQuestions\\\n",
    "Yeah bitch, yeah bitch, call me _______:\\\n",
    "a.) Maybe\\\n",
    "b.) Steve-O\\\n",
    "c.) Later\\\n",
    "d.) Jesse Pinkman\"\"\\\n",
    "@machinegunkelly\"\n",
    "\n",
    "#### **Username Cleanup**\n",
    "\n",
    "I chose to just drop all usernames.  This is due to usernames being random characters and it's out of scope for this project to try to do abusive username detection.\n",
    "\n",
    "#### **Unicode Characters, numbers, and non-text characters**\n",
    "\n",
    "I also chose to drop all unicode characters (Doing this save soo much time), numbers and most non-text characters.  The exception was the \"*\" character.  The * character is used to mask offensive words and when used, the resulting word length is fairly standard.\n",
    "\n",
    "# Initial EDA\n",
    "The average character count of the uncleaned tweets is 85.  The average word count is 14.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Charater Count: 85.43606504458701\n",
      "Mean Word Count: 14.07097607230763\n",
      "Basic Data CharasiticsCounter({1: 19190, 2: 4163, 0: 1430})\n",
      "30 Most common words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 9099),\n",
       " ('RT', 7539),\n",
       " ('bitch', 6638),\n",
       " ('the', 6590),\n",
       " ('I', 6472),\n",
       " ('to', 5240),\n",
       " ('you', 4881),\n",
       " ('and', 3670),\n",
       " ('that', 3111),\n",
       " ('my', 3072),\n",
       " ('in', 2902),\n",
       " ('is', 2759),\n",
       " ('bitches', 2576),\n",
       " ('like', 2534),\n",
       " ('of', 2503),\n",
       " ('on', 2361),\n",
       " ('be', 2304),\n",
       " ('me', 2249),\n",
       " ('for', 2023),\n",
       " ('hoes', 1925),\n",
       " ('with', 1778),\n",
       " ('pussy', 1711),\n",
       " ('this', 1575),\n",
       " (\"I'm\", 1496),\n",
       " ('hoe', 1470),\n",
       " ('ass', 1447),\n",
       " ('it', 1445),\n",
       " ('your', 1423),\n",
       " ('get', 1328),\n",
       " ('up', 1313)]"
      ]
     },
     "execution_count": 25,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean Charater Count: \" + str(statistics.mean(raw_data['tweet'].str.len())))\n",
    "print(\"Mean Word Count: \" + str(np.mean(raw_data['tweet'].apply(lambda x: len([words for words in x.split(\" \") if isinstance(x, str)])))))\n",
    "print(\"Basic Data Charasitics\" + str(Counter(raw_data['class'])))\n",
    "print(\"30 Most common words.\")\n",
    "Counter(\" \".join(raw_data['tweet']).split()).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         rt as a woman you shouldn t complain about cl...\n",
      "1         rt boy dats cold tyga dwn bad for cuffin dat ...\n",
      "2         rt you ever fuck a bitch and she start to cry...\n",
      "3                     rt viva based she look like a tranny\n",
      "4         rt the shit you hear about me might be true o...\n",
      "                               ...                        \n",
      "24778    you s a muthaf***in lie pearls corey emanuel r...\n",
      "24779    you ve gone and broke the wrong heart baby and...\n",
      "24780    young buck wanna eat dat nigguh like i aint fu...\n",
      "24781                youu got wild bitches tellin you lies\n",
      "24782     ruffled ntac eileen dahlia beautiful color co...\n",
      "Name: tweet, Length: 24783, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def text_cleanup(text):\n",
    "    #Remove usernames replace @xxx: with whitespace\n",
    "    text = re.sub(\"@.*?:\", \" \", text)\n",
    "    #remove unicode characters\n",
    "    text = re.sub(\"&#[0-9]{1,6};\", \" \", text)\n",
    "    #lowercase all text\n",
    "    text = text.lower()\n",
    "    #remove URL's\n",
    "    text = re.sub('http[s]?:\\/\\/.*?\"', \" \", text)\n",
    "    #remove special characters and numberrs'\n",
    "    #I chose to leave in *'s as they are a common manipulation to get around filters\n",
    "    text = re.sub(\"[^a-z\\*]\", \" \", text)\n",
    "    #remove repeat whitespace and newlines\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "    text = re.sub(\"\\n+\", \"\", text)\n",
    "    return text\n",
    "raw_data[\"tweet\"] = raw_data[\"tweet\"].apply(text_cleanup)\n",
    "print(raw_data['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## EDA Procedure\n",
    "\n",
    "### Basic Data Charaistics After Cleanup\n",
    "\n",
    "A tweet at the time of this data being collected can contain at maxiumum 280 Characters.  \n",
    "Mean Char Count\n",
    "Mean Word Count\n",
    "\n",
    "### Formatting Data\n",
    "\n",
    "I will start with dropping all the non-tweet columns.  This will be blind learning.\n",
    "\n",
    "### Bagging\n",
    "\n",
    "I chose to use TF\\-IDF for my bagging procedure.  I am testing using unigrams single words only.  I may retest this using bigrams later., and am removing common English stopwords.  I decided not to restrict common words as the most common words are 27%.  I may experiment with this later.  \n",
    "\n",
    "Exploratory analays was done on unigrams and bigrams.  \n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "DBSCAN or Density\\-Based Spatial Clustering of Applications with Noise is a clustering algorithm that uses the density of datapoints to classify objects.  In this case, I'm using DBSCAN to provide coloring of my graphed results.  Although I will use it later for actual model building, I am using it here to provide color differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That's not good.  All of the data seems to be in 1 large clump.  That will not do for classification.  I can try to get rid of the outliers, or I can try to clean the data.  I will try both. \n",
    "\n",
    "## Attempt to get rid of highly common elements.\n",
    "\n",
    "In this I will rerun the TF\\-IDF with a maximum document frequency of .2 and .15 and see if I get a better spread.  This change reduced the spacing between the 2 sets of elements.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def color_graph(model_data, plot_title = \"\"):\\\n",
    "    grid = sns.FacetGrid(data =model_data,  col = 'gram_size', row = 'k').set(title = plot_title)\n",
    "    grid.map(sns.scatterplot(\n",
    "        x=model_data['x'].tolist(), y=model_data['y'].tolist(),\n",
    "        hue=model_data['predict'].tolist(),\n",
    "        legend=\"full\",\n",
    "        alpha=0.3\n",
    "    ))\n",
    "    print(model_data['predict'].value_counts())\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def TF_IDF(tweets, ngram_range, max_df):\n",
    "    tfidf = TfidfVectorizer(input='content', encoding='utf-8', \\\n",
    "                            decode_error='strict', strip_accents='ascii', \\\n",
    "                            lowercase=True, preprocessor=None, tokenizer=None, \\\n",
    "                            analyzer='word', stop_words='english',  \\\n",
    "                            ngram_range=(ngram_range, ngram_range),max_df = max_df, max_features=None,\\\n",
    "                            vocabulary=None, binary=False, norm='l2', \\\n",
    "                            use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "    tfidf = tfidf.fit(raw_data['tweet'])\n",
    "    tfidf_data = tfidf.transform(raw_data['tweet'])\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    return (tfidf_data, feature_names)\n",
    "\n",
    "def TSNE_Process(tfidf_data, gram_size, k):\n",
    "    tsne_results = TSNE(n_components=2, learning_rate='auto'\\\n",
    "                                 ,init='random', perplexity=30, n_iter = 600\\\n",
    "                                ).fit_transform(tfidf_data)\n",
    "    tsne_results = pd.DataFrame(tsne_results, columns = ['x', 'y'])\n",
    "    tsne_results['gram_size'] = gram_size\n",
    "    tsne_results['k'] = k\n",
    "    return tsne_results\n",
    "def build_model(tsne_results):\n",
    "    dbscan = DBSCAN(eps=2.0, min_samples=100,\\\n",
    "                    metric='euclidean', metric_params=None, \\\n",
    "                    algorithm='auto', leaf_size=30, p=None, \\\n",
    "                    n_jobs=None)\n",
    "    model = dbscan.fit(tsne_results)\n",
    "    model_data['predict'] = model.fit_predict(model_data,model_data)\n",
    "    model_data['predict'] = model_data['predict'].where(model_data['predict'] == 0, 1)\n",
    "    #print(model_data['predict'].value_counts())\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Unigram'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_258/3016953909.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0munigram_k_1_TF_IDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigram_k_1_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTF_IDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0munigram_k_1_TSNE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE_Process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigram_k_1_TF_IDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unigram'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigram_k_1_TSNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0munigram_k_15_TF_IDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigram_k_15_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTF_IDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0munigram_k_15_TSNE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE_Process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigram_k_15_TF_IDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unigram'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_258/661094574.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(tsne_results)\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     n_jobs=None)\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsne_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mmodel_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mmodel_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 )\n\u001b[1;32m   1150\u001b[0m             ):\n\u001b[0;32m-> 1151\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_dbscan.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \"\"\"\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1996\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         if (\n\u001b[1;32m   2000\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Unigram'"
     ]
    }
   ],
   "source": [
    "model_data = pd.DataFrame()\n",
    "unigram_k_1_TF_IDF, unigram_k_1_features = TF_IDF(raw_data['tweet'], 1, 1)\n",
    "unigram_k_1_TSNE = TSNE_Process(unigram_k_1_TF_IDF, 'Unigram', 1)\n",
    "model_data = pd.concat([build_model(unigram_k_1_TSNE), model_data])\n",
    "unigram_k_15_TF_IDF, unigram_k_15_features = TF_IDF(raw_data['tweet'], 1, .15)\n",
    "unigram_k_15_TSNE = TSNE_Process(unigram_k_15_TF_IDF, 'Unigram', .15)\n",
    "model_data = pd.concat([build_model(unigram_k_15_TSNE), model_data])\n",
    "\n",
    "bigram_k_1_TF_IDF, bigram_k_1_features = TF_IDF(raw_data['tweet'], 2, 1)\n",
    "bigram_k_1_TSNE = TSNE_Process(bigram_k_1_TF_IDF, 'Bigram', 1)\n",
    "model_data = pd.concat([build_model(bigram_k_1_Model), model_data])\n",
    "bigram_k_15_TF_IDF, bigram_k_15_features = TF_IDF(raw_data['tweet'], 2, .15)\n",
    "bigram_k_15_TSNE = TSNE_Process(bigram_k_15_TF_IDF, 'Bigram', '.15')\n",
    "model_data = pd.concat([build_model(bigram_k_15_TSNE), model_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TSNE Results\n",
    "\n",
    "TSNE using both unigrams and bigrams seems to have the same results.  A ring with some spread data and a very tight core.  Just looking at some random results from each category, the groupings might be a division of hate speech vs offensive language.  I will try to match these 2 categories and check the performance.  I will only use correct answers from each category.  While categorizing the data is not my eventual goal, I want to see what happens with data this dimensionolly reduced.\n",
    "\n",
    "### Scores\n",
    "\n",
    "I used simple Precision, Recall, and F1 Scores.  With this data the bigrams did surprisingly well, with an 82% weighted Precision and a raw accuracy of 70%.  Note:  The weighted scores may be innacurate with no 2 values in the dataset.\n",
    "\n",
    "### EDA Results\n",
    "\n",
    "Even though I used an ML algorighim to perform striping, I still consider this EDA as I used basic tequniqes to find a pattern before performing more aggressvie Unstructured Learning.  I found that just paring down the data to 2 columns with bigrams I can create a model with a  70% precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "color_graph(model_data, \"EDA TSNE Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def makeScores(d1, d2, text):\n",
    "    print(text + \":  Raw Precision: \" + str(precision_score(d1, d2, average = 'micro')) + \\\n",
    "          \", Weighted Precision: \" + str(precision_score(d1, d2, average = 'weighted')) + \\\n",
    "         \", Weighted Recall: \" + str(recall_score(d1, d2, average = 'weighted')) +\\\n",
    "         \", Weghted F1: \" + str(f1_score(d1, d2, average = 'weighted')))\n",
    "makeScores(unigram_k_1_Model['predict'], raw_data['class'], \"Unigram k = 1\")\n",
    "makeScores(unigram_k_15_Model['predict'], raw_data['class'], \"Unigram k = .15\")\n",
    "makeScores(bigram_k_1_Model['predict'], raw_data['class'], \"Bigram k = 1\")\n",
    "makeScores(bigram_k_15_Model['predict'], raw_data['class'], \"Bigram k = .15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Building\n",
    "I'm going to start with a DBSCAN model.  This is a popular model for text categorization and with the large doughnut hole in the middle of the data, I'm going to see if I can get the 2 blocks of data and test the results.\n",
    "### DBSCAN\n",
    "DBSCAN or Density-Based Spatial Clustering of Applications with Noise is a clustering algorithm that uses the density of datapoints to classify objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}