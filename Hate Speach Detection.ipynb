{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hate Speech Detection\n",
    "\n",
    "I want to build an algorithim to  categorize tweets.  This will use the Hate Speach and Offensive Language dataset at [https://www.kaggle.com/datasets/mrmorj/hate\\-speech\\-and\\-offensive\\-language\\-dataset?select=labeled\\_data.csv](https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset?select=labeled_data.csv).  This dataset is 24782 tweets long with users voting on if the tweet is offensive, hate\\_speach or neither, with the winning label being selected as a class.  \n",
    "\n",
    "## Goal\n",
    "\n",
    "My goal is to identify groupings of tweets based on topic.  With 27k+  tweets, I should be able to find 30-50 groups based on topics.\n",
    "\n",
    "## Approach\n",
    "\n",
    "I am planning to use a combination of TF-IDF, matrix factorization, and clustering algorithims to tweets by 1 and 2 word clusters.  I will then create similarity tables and use rainbow clustering to find groups of tweets.\n",
    "\n",
    "### Disclaimer\n",
    "\n",
    "This assignment necessary contains hate speach, and offensive langauge.  This does not represent the views of the author.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import statistics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0               0      3            0                   0        3      2   \n",
      "1               1      3            0                   3        0      1   \n",
      "2               2      3            0                   3        0      1   \n",
      "3               3      3            0                   2        1      1   \n",
      "4               4      6            0                   6        0      1   \n",
      "...           ...    ...          ...                 ...      ...    ...   \n",
      "24778       25291      3            0                   2        1      1   \n",
      "24779       25292      3            0                   1        2      2   \n",
      "24780       25294      3            0                   3        0      1   \n",
      "24781       25295      6            0                   6        0      1   \n",
      "24782       25296      3            0                   0        3      2   \n",
      "\n",
      "                                                   tweet  \n",
      "0      !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "...                                                  ...  \n",
      "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...  \n",
      "24779  you've gone and broke the wrong heart baby, an...  \n",
      "24780  young buck wanna eat!!.. dat nigguh like I ain...  \n",
      "24781              youu got wild bitches tellin you lies  \n",
      "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...  \n",
      "\n",
      "[24783 rows x 7 columns]\n",
      "85.43606504458701\n"
     ]
    }
   ],
   "source": [
    "#Initial Import\n",
    "raw_data = pd.read_csv('Data/labeled_data.csv', header = 0)\n",
    "print(raw_data)\n",
    "raw_data = raw_data.drop(columns = 'Unnamed: 0')\n",
    "print(statistics.mean(raw_data['tweet'].str.len()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import and Cleaning\n",
    "\n",
    "### Import\n",
    "\n",
    "This data imports fairly easily.  I donloaded a copy of the file from kaggle and mirrored it to my GitHub so I could autodownload it for your use.\n",
    "\n",
    "### Data Description\n",
    "The data concists of 24782 tweets which have been user labeled as either hate, offensive, or neither, along with a total cont of votes.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "#### **Tweets with newline**\n",
    "\n",
    "There appears to be a mismatch between the number of lines generated and the number of lines imported.  This is caused by newline characters, commas, quotation marks, and other supidity mixed in.  I chose to build my own parser to deal with all the issues and allow me to deal with the various problems with the data.\\\n",
    "**Update**\\\n",
    "There are skipped tweet numbers with no indicator in the dataset other than looking.\n",
    "\n",
    "##### **Some examples of problematic formating.**\n",
    "\n",
    "22422,3,0,3,0,1,\"This bitch fell straight through the chair... Im high, of course I laughed..\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "Even if I wasn't I would've still laughed.\"\n",
    "\n",
    "256,3,0,3,0,1,\"\"\"@TheNewSAT: #NewSATQuestions\\\n",
    "Yeah bitch, yeah bitch, call me _______:\\\n",
    "a.) Maybe\\\n",
    "b.) Steve-O\\\n",
    "c.) Later\\\n",
    "d.) Jesse Pinkman\"\"\\\n",
    "@machinegunkelly\"\n",
    "\n",
    "#### **Username Cleanup**\n",
    "\n",
    "I chose to just drop all usernames.  This is due to usernames being random characters and it's out of scope for this project to try to do abusive username detection.\n",
    "\n",
    "#### **Unicode Characters, numbers, and non-text characters**\n",
    "\n",
    "I also chose to drop all unicode characters (Doing this save soo much time), numbers and most non-text characters.  The exception was the \"*\" character.  The * character is used to mask offensive words and when used, the resulting word length is fairly standard.\n",
    "\n",
    "# Initial EDA\n",
    "The average character count of the uncleaned tweets is 85.  The average word count is 14.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Charater Count: 85.43606504458701\n",
      "Mean Word Count: 14.07097607230763\n",
      "30 Most common words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 9099),\n",
       " ('RT', 7539),\n",
       " ('bitch', 6638),\n",
       " ('the', 6590),\n",
       " ('I', 6472),\n",
       " ('to', 5240),\n",
       " ('you', 4881),\n",
       " ('and', 3670),\n",
       " ('that', 3111),\n",
       " ('my', 3072),\n",
       " ('in', 2902),\n",
       " ('is', 2759),\n",
       " ('bitches', 2576),\n",
       " ('like', 2534),\n",
       " ('of', 2503),\n",
       " ('on', 2361),\n",
       " ('be', 2304),\n",
       " ('me', 2249),\n",
       " ('for', 2023),\n",
       " ('hoes', 1925),\n",
       " ('with', 1778),\n",
       " ('pussy', 1711),\n",
       " ('this', 1575),\n",
       " (\"I'm\", 1496),\n",
       " ('hoe', 1470),\n",
       " ('ass', 1447),\n",
       " ('it', 1445),\n",
       " ('your', 1423),\n",
       " ('get', 1328),\n",
       " ('up', 1313)]"
      ]
     },
     "execution_count": 5,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean Charater Count: \" + str(statistics.mean(raw_data['tweet'].str.len())))\n",
    "print(\"Mean Word Count: \" + str(np.mean(raw_data['tweet'].apply(lambda x: len([words for words in x.split(\" \") if isinstance(x, str)])))))\n",
    "print(\"30 Most common words.\")\n",
    "Counter(\" \".join(raw_data['tweet']).split()).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         rt as a woman you shouldn t complain about cl...\n",
      "1         rt boy dats cold tyga dwn bad for cuffin dat ...\n",
      "2         rt you ever fuck a bitch and she start to cry...\n",
      "3                     rt viva based she look like a tranny\n",
      "4         rt the shit you hear about me might be true o...\n",
      "                               ...                        \n",
      "24778    you s a muthaf***in lie pearls corey emanuel r...\n",
      "24779    you ve gone and broke the wrong heart baby and...\n",
      "24780    young buck wanna eat dat nigguh like i aint fu...\n",
      "24781                youu got wild bitches tellin you lies\n",
      "24782     ruffled ntac eileen dahlia beautiful color co...\n",
      "Name: tweet, Length: 24783, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def text_cleanup(text):\n",
    "    #Remove usernames replace @xxx: with whitespace\n",
    "    text = re.sub(\"@.*?:\", \" \", text)\n",
    "    #remove unicode characters\n",
    "    text = re.sub(\"&#[0-9]{1,6};\", \" \", text)\n",
    "    #lowercase all text\n",
    "    text = text.lower()\n",
    "    #remove URL's\n",
    "    text = re.sub('http[s]?:\\/\\/.*?\"', \" \", text)\n",
    "    #remove special characters and numberrs'\n",
    "    #I chose to leave in *'s as they are a common manipulation to get around filters\n",
    "    text = re.sub(\"[^a-z\\*]\", \" \", text)\n",
    "    #remove repeat whitespace and newlines\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "    text = re.sub(\"\\n+\", \"\", text)\n",
    "    return text\n",
    "raw_data[\"tweet\"] = raw_data[\"tweet\"].apply(text_cleanup)\n",
    "print(raw_data['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## EDA Procedure\n",
    "### Basic Data Charaistics After Cleanup\n",
    "A tweet at the time of this data being collected can contain at maxiumum 280 Characters.  \n",
    "Mean Char Count\n",
    "Mean Word Count\n",
    "### Formatting Data\n",
    "I will start with dropping all the non-tweet columns.  This will be blind learning.\n",
    "### Bagging\n",
    "I chose to use TF-IDF for my bagging procedure.  I am testing using unigrams single words only.  I may retest this using bigrams later., and am removing common english stopwords.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "#TF IDF\n",
    "tfidf = TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents='ascii', lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words='english',  ngram_range=(1, 1), max_features=None, vocabulary=None, binary=False, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "tfidf = tfidf.fit(raw_data['tweet'])\n",
    "tfidf_data = tfidf.transform(raw_data['tweet'])\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "#https://stackoverflow.com/questions/34232190/scikit-learn-tfidfvectorizer-how-to-get-top-n-terms-with-highest-tf-idf-score\n",
    "\n",
    "tfidf_sorting = np.argsort(tfidf_data.toarray()).flatten()[::-1]\n",
    "n = 10\n",
    "top_n = feature_names[tfidf_sorting][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}