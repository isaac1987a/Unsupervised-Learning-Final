{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e3e17d-dca0-47fb-bb08-b937506f2ec7",
   "metadata": {},
   "source": [
    "# Hate Speach Detection\n",
    "I want to build an algorithim to detect hate speach.  This will use the Hate Speach and Offensive Language dataset at https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset?select=labeled_data.csv.  This dataset is 24782 tweets long with users voting on if the tweet is offensive, hate_speach or neither, with the winning label being selected as a class.\n",
    "## Approach\n",
    "I will approach this as a text classification problem similar to the BBC problem.  The big difference is that my text can be labeled as either hate speach, or offensive language.  Users were allowed to vote on which category the data fell into but were only permitted to select 1 category when multiple may apply.\n",
    "\n",
    "I'm going to move the count to a boolean selection.  If either hate speach or offensive language have more than 1/3 of the count, it will be 1, otherwise it will be 0.  After I perform NMF, I will use various models to bucket the new text.  For classification, I will generate an expected value and test various cutoffs for classification.\n",
    "\n",
    "### Disclaimer\n",
    "This assignment necissarly contains hate speach, and offensive langauge.  This does not represent the views of the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c222a22a-d816-4d91-b317-0908f884f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7927248-8812-484d-97e0-e26c4530a697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0               0      3            0                   0        3      2   \n",
      "1               1      3            0                   3        0      1   \n",
      "2               2      3            0                   3        0      1   \n",
      "3               3      3            0                   2        1      1   \n",
      "4               4      6            0                   6        0      1   \n",
      "...           ...    ...          ...                 ...      ...    ...   \n",
      "24778       25291      3            0                   2        1      1   \n",
      "24779       25292      3            0                   1        2      2   \n",
      "24780       25294      3            0                   3        0      1   \n",
      "24781       25295      6            0                   6        0      1   \n",
      "24782       25296      3            0                   0        3      2   \n",
      "\n",
      "                                                   tweet  \n",
      "0      !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "...                                                  ...  \n",
      "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...  \n",
      "24779  you've gone and broke the wrong heart baby, an...  \n",
      "24780  young buck wanna eat!!.. dat nigguh like I ain...  \n",
      "24781              youu got wild bitches tellin you lies  \n",
      "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...  \n",
      "\n",
      "[24783 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Initial Import\n",
    "raw_data = pd.read_csv('C:\\\\Users\\\\isaac\\\\OneDrive\\\\School\\\\'\\\n",
    "           'MS\\\\Unstructured Learning\\\\Week 5\\\\'\\\n",
    "           'Unsupervised-Learning-Final\\\\Data\\\\'\\\n",
    "           'labeled_data.csv', header = 0)\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0eebb-45e3-4012-9973-88a5b302d2b1",
   "metadata": {},
   "source": [
    "## Import and Cleaning\n",
    "### Import\n",
    "This data imports fairly easily.  I donloaded a copy of the file from kaggle and mirrored it to my GitHub so I could autodownload it for your use.\n",
    "### Cleaning\n",
    "#### **Tweets with newline**\n",
    "There appears to be a mismatch between the number of lines generated and the number of lines imported.  This is caused by newline characters, commas, quotation marks, and other supidity mixed in.  I chose to build my own parser to deal with all the issues and allow me to deal with the various problems with the data.\\\n",
    "**Update**\\\n",
    "There are skipped tweet numbers with no indicator in the dataset other than looking.\n",
    "##### **Some examples of problematic formating.**\n",
    "22422,3,0,3,0,1,\"This bitch fell straight through the chair... Im high, of course I laughed..\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "Even if I wasn't I would've still laughed.\"\n",
    "\n",
    "256,3,0,3,0,1,\"\"\"@TheNewSAT: #NewSATQuestions\\\n",
    "Yeah bitch, yeah bitch, call me _______:\\\n",
    "a.) Maybe\\\n",
    "b.) Steve-O\\\n",
    "c.) Later\\\n",
    "d.) Jesse Pinkman\"\"\\\n",
    "@machinegunkelly\"\n",
    "\n",
    "\n",
    "#### **Username Cleanup**\n",
    "I chose to just drop all usernames.  This is due to usernames being random characters and it's out of scope for this project to try to do abusive username detection.\n",
    "#### **Unicode Characters, numbers, and non-text characters**\n",
    "I also chose to drop all unicode characters, numbers and most non-text characters.  The exception was the \"*\" character.  The * character is used to mask offensive words and when used, the resulting word length is fairly standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5093c7e-fbd8-468e-ab66-ed5ff5cfb432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0               0      3            0                   0        3      2   \n",
      "1               1      3            0                   3        0      1   \n",
      "2               2      3            0                   3        0      1   \n",
      "3               3      3            0                   2        1      1   \n",
      "4               4      6            0                   6        0      1   \n",
      "...           ...    ...          ...                 ...      ...    ...   \n",
      "24778       25291      3            0                   2        1      1   \n",
      "24779       25292      3            0                   1        2      2   \n",
      "24780       25294      3            0                   3        0      1   \n",
      "24781       25295      6            0                   6        0      1   \n",
      "24782       25296      3            0                   0        3      2   \n",
      "\n",
      "                                                   tweet  \n",
      "0       rt as a woman you shouldn t complain about cl...  \n",
      "1       rt boy dats cold tyga dwn bad for cuffin dat ...  \n",
      "2       rt you ever fuck a bitch and she start to cry...  \n",
      "3                   rt viva based she look like a tranny  \n",
      "4       rt the shit you hear about me might be true o...  \n",
      "...                                                  ...  \n",
      "24778  you s a muthaf***in lie pearls corey emanuel r...  \n",
      "24779  you ve gone and broke the wrong heart baby and...  \n",
      "24780  young buck wanna eat dat nigguh like i aint fu...  \n",
      "24781              youu got wild bitches tellin you lies  \n",
      "24782   ruffled ntac eileen dahlia beautiful color co...  \n",
      "\n",
      "[24783 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "def text_cleanup(text):\n",
    "    #Remove usernames replace @xxx: with whitespace\n",
    "    text = re.sub(\"@.*?:\", \" \", text)\n",
    "    #remove unicode characters\n",
    "    text = re.sub(\"&#[0-9]{6};\", \" \", text)\n",
    "    #lowercase all text\n",
    "    text = text.lower()\n",
    "    #remove URL's\n",
    "    text = re.sub('http[s]?:\\/\\/.*?\"', \" \", text)\n",
    "    #remove special characters and numberrs'\n",
    "    #I chose to leave in *'s as they are a common manipulation to get around filters\n",
    "    text = re.sub(\"[^a-z\\*]\", \" \", text)\n",
    "    #remove repeat whitespace and newlines\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "    text = re.sub(\"\\n+\", \"\", text)\n",
    "    return text\n",
    "raw_data[\"tweet\"] = raw_data[\"tweet\"].apply(text_cleanup)\n",
    "print(raw_data)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b83e21c-87b7-4108-a875-e520d58d13c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tweet num'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tweet num'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Find why I still only have 24783 coumns when the dataset says it has 25.3k\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[43mraw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet num\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[i]) \u001b[38;5;241m!=\u001b[39m i:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m (i)\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tweet num'"
     ]
    }
   ],
   "source": [
    "#Find why I still only have 24783 coumns when the dataset says it has 25.3k\n",
    "for i in range(0,1000):\n",
    "    if int(raw_data['tweet num'][i]) != i:\n",
    "        print (i)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5654180a-cb07-49e9-818e-c41bfa86207d",
   "metadata": {},
   "source": [
    "## EDA Procedure\n",
    "### Formatting Data\n",
    "I started with formatting my data so that hate speach, offensive language values are 1 if they have at least 1/3 of the votes, otherwise they will be 0.\n",
    "### Bagging\n",
    "I chose to use TF-IDF for my bagging procedure.  I will do my testing while removing common english stopwords.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93127158-5bb9-48c5-b597-b2c7dc22bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF IDF\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
